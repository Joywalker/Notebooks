{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Shape Classification using MobileNetV2 and Tensorflow\n![](https://www.researchgate.net/publication/342856036/figure/fig3/AS:911929400885251@1594432320422/The-architecture-of-the-MobileNetv2-network.ppm)\n\nIn order to download, I had to set an access token for Kaggle API and install needed python packages. Set-up kaggle access token json file, and allow full read and write access to the file, while no other user can access the file using chmod 600 code.","metadata":{}},{"cell_type":"code","source":"# Setting up access to kaggle api\n# In order to generate kaggle.json file, go to your kaggle profile and click Generate API key.\n# It will download a json file with the followings username: X, key : Y\n# Replace those two values in the following printf line\n! printf '{\"username\":\"razvantalexandru\", \"key\":\"d43f87acf66f1b73c65c17d74cae5f55\"}' > /root/.kaggle/kaggle.json\n! chmod 600 /root/.kaggle/kaggle.json\n\n# Download the dataset\n! rm -rf dataset\n! kaggle datasets download -d niten19/face-shape-dataset\n! unzip face-shape-dataset.zip -d dataset\n! mv dataset/'FaceShape Dataset'/training_set dataset/train\n! mv dataset/'FaceShape Dataset'/testing_set dataset/test\n! rm -rf dataset/'FaceShape Dataset'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define imports\nimport os\nimport tensorflow as tf\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ntf.config.list_physical_devices('GPU')\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom struct import unpack\nfrom PIL import ImageFile\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom keras.applications.inception_v3 import preprocess_input\n\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\n# Additional definitions\nImageFile.LOAD_TRUNCATED_IMAGES = True\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\n# Define config params and Hyperparams\nDEBUG = False\nEPOCHS = 100\nLR = 1e-3\nDATASET_SEED = 1337\nIMAGE_SIZE = 224\nBATCH_SIZE = 8\nDS_FROM_DIR = False\n\n\n# Define needed paths\nBASE_DS_PATH = 'dataset'\nTRAIN_SPLIT_PATH = os.path.join(BASE_DS_PATH, 'train')\nEVAL_SPLIT_PATH = os.path.join(BASE_DS_PATH, 'test')\nCHKPT_FILEPATH = 'checkpoints/best.model'\n\n# --------- Setup classes -----------\nclasses = {\n    0: 'Heart',\n    1: 'Oblong',\n    2: 'Oval',\n    3: 'Round',\n    4: 'Square'\n}\n\nNUM_CLASSES = len(classes.keys())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clearing dataset for corrupted samples\nWhile analyzing, some samples could not be feed-forwarded through the net because of the corrupt format (bad Int format).\nThis code-cell will check the dataset and remove such samples.","metadata":{}},{"cell_type":"code","source":"#----------CHECK DATASET FOR INCONSISTIENCES----------\n# Some of the images in this dataset seem to be corrupted, so run this JPEG binary checker to remove those hazards.\nclass JPEG:\n    def __init__(self, image_file):\n        with open(image_file, 'rb') as f:\n            self.img_data = f.read()\n    \n    def decode(self):\n        data = self.img_data\n        while(True):\n            marker, = unpack(\">H\", data[0:2])\n            # print(marker_mapping.get(marker))\n            if marker == 0xffd8:\n                data = data[2:]\n            elif marker == 0xffd9:\n                return\n            elif marker == 0xffda:\n                data = data[-2:]\n            else:\n                lenchunk, = unpack(\">H\", data[2:4])\n                data = data[2+lenchunk:]            \n            if len(data)==0:\n                break   \n# JPEG binary marker mappings (verify integrity of jpeg file)\nmarker_mapping = {\n    0xffd8: \"Start of Image\",\n    0xffe0: \"Application Default Header\",\n    0xffdb: \"Quantization Table\",\n    0xffc0: \"Start of Frame\",\n    0xffc4: \"Define Huffman Table\",\n    0xffda: \"Start of Scan\",\n    0xffd9: \"End of Image\"\n}\n\n\nassert os.path.exists(BASE_DS_PATH), 'Given path does not exist. Provide a valid one'\ncorrupted = []\n\nfor root, subdirectories, files in tqdm(os.walk(BASE_DS_PATH)):\n    for im_file in files:\n        file_path = os.path.join(root, im_file)\n        try:\n            image = JPEG(file_path) \n            image.decode()   \n        except:\n          # Image is corrupted -> removing\n            if os.path.exists(file_path):\n            os.remove(file_path)\n            print(f\"Removed {file_path} - Corrupted image\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data generators\nAdd data augmentations:\n- rescaling [0,1]\n- rotation around Z (backward axis) or plain rotation in 2D (clockwise)\n- zooming \n- horizontal (random H flips)","metadata":{}},{"cell_type":"code","source":"# Setup data-generators\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1./255., \n    rotation_range = 40, \n    zoom_range = 0.2, \n    horizontal_flip = True\n)\n\nvalid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n   rescale = 1./255.\n)\n\n\n# Flow from directory data loaders\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_SPLIT_PATH,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    target_size=(IMAGE_SIZE, IMAGE_SIZE)\n)\nvalid_generator = valid_datagen.flow_from_directory(\n    EVAL_SPLIT_PATH,\n    class_mode='categorical',\n    batch_size=BATCH_SIZE,\n    target_size=(IMAGE_SIZE, IMAGE_SIZE))\n\nif DEBUG:\n  # Visualize a few imagess\n  plt.figure(figsize=(10, 10))\n    for images, labels in train_datagen.take(1):\n        for i in range(100):\n            ax = plt.subplot(3, 3, i + 1)\n            plt.imshow(images[i].numpy().astype(\"uint8\"))\n            plt.title(int(labels[i]))\n            plt.axis(\"off\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model training loop\n- *Define optimizer* - Adam (converges faster and is the best choice considering the loss criterion\n- *Define loss* - Categorical Crossentropy (since we're training multiple classes, categorical cross entropy is the recommended loss function. We don't use sparse categorical because the output logits were already processed (softmax layer).\n- *Define callback* - model checkpoint saver","metadata":{}},{"cell_type":"code","source":"# Load efficient-net-b7 from keras\nmodel = tf.keras.applications.MobileNetV2(\n    include_top=False,\n    weights=\"imagenet\",\n    input_tensor=None,\n    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n    pooling=None,\n    classes=NUM_CLASSES\n)\n# Freeze backbone\nfor layer in model.layers:\n    layer.trainable = False\n\n# Add prediction head adapted to our usecase\nbase = model.output\ninterm_logits = tf.keras.layers.Flatten()(base) # (2048,)\ninterm_logits = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='l2')(interm_logits)\ninterm_logits = tf.keras.layers.Dropout(0.4)(interm_logits)\ninterm_logits = tf.keras.layers.Dense(256, activation='relu')(interm_logits)\noutputs = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(interm_logits)\n\n\n# Define new model with custom head\nmodel = tf.keras.Model(inputs=model.input, outputs=outputs)\n\n# Define checkpoint callback\nchkpt_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath= \"checkpoints\",\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True,\n    frequency=10)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile model with sparse-categorical (multi-class)\nmodel.compile(\n    optimizer='adam',\n    loss= 'categorical_crossentropy',\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(\n  train_generator,\n  validation_data=valid_generator,\n  epochs=EPOCHS,\n  callbacks=[chkpt_callback]\n)\n\nmodel.save(\"saved_model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the training charts\n2 matplotlib charts describing:\n- Plots the accuracy and validation accuracy\n- Plots the loss and the validation loss","metadata":{}},{"cell_type":"code","source":"# See training stats\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}
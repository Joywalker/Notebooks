{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Shape Classification using MobileNetV2 and Tensorflow\n![](https://www.researchgate.net/publication/342856036/figure/fig3/AS:911929400885251@1594432320422/The-architecture-of-the-MobileNetv2-network.ppm)\n\nIn order to download, I had to set an access token for Kaggle API and install needed python packages. Set-up kaggle access token json file, and allow full read and write access to the file, while no other user can access the file using chmod 600 code.","metadata":{}},{"cell_type":"code","source":"# Setting up access to kaggle api\n! printf '{\"username\":\"razvantalexandru\", \"key\":\"d43f87acf66f1b73c65c17d74cae5f55\"}' > /root/.kaggle/kaggle.json\n! chmod 600 /root/.kaggle/kaggle.json\n\n# Download the dataset\n! rm -rf dataset\n! kaggle datasets download -d niten19/face-shape-dataset\n! unzip face-shape-dataset.zip -d dataset\n! mv dataset/'FaceShape Dataset'/training_set dataset/train\n! mv dataset/'FaceShape Dataset'/testing_set dataset/test\n! rm -rf dataset/'FaceShape Dataset'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up access to kaggle api\n! printf '{\"username\":\"razvantalexandru\", \"key\":\"d43f87acf66f1b73c65c17d74cae5f55\"}' > /root/.kaggle/kaggle.json\n! chmod 600 /root/.kaggle/kaggle.json\n\n# Download the dataset\n! rm -rf dataset\n! kaggle datasets download -d niten19/face-shape-dataset\n! unzip face-shape-dataset.zip -d dataset\n! mv dataset/'FaceShape Dataset'/training_set dataset/train\n! mv dataset/'FaceShape Dataset'/testing_set dataset/test\n! rm -rf dataset/'FaceShape Dataset'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define imports\nimport os\nimport tensorflow as tf\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ntf.config.list_physical_devices('GPU')\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom struct import unpack\nfrom PIL import ImageFile\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom keras.applications.inception_v3 import preprocess_input\n\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\n# Additional definitions\nImageFile.LOAD_TRUNCATED_IMAGES = True\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\n# Define config params and Hyperparams\nDEBUG = False\nEPOCHS = 100\nLR = 1e-3\nDATASET_SEED = 1337\nIMAGE_SIZE = 224\nBATCH_SIZE = 8\nDS_FROM_DIR = False\n\n\n# Define needed paths\nBASE_DS_PATH = 'dataset'\nTRAIN_SPLIT_PATH = os.path.join(BASE_DS_PATH, 'train')\nEVAL_SPLIT_PATH = os.path.join(BASE_DS_PATH, 'test')\nCHKPT_FILEPATH = 'checkpoints/best.model'\n\n# --------- Setup classes -----------\nclasses = {\n    0: 'Heart',\n    1: 'Oblong',\n    2: 'Oval',\n    3: 'Round',\n    4: 'Square'\n}\n\nNUM_CLASSES = len(classes.keys())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#----------CHECK DATASET FOR INCONSISTIENCES----------\nclass JPEG:\n    def __init__(self, image_file):\n        with open(image_file, 'rb') as f:\n            self.img_data = f.read()\n    \n    def decode(self):\n        data = self.img_data\n        while(True):\n            marker, = unpack(\">H\", data[0:2])\n            # print(marker_mapping.get(marker))\n            if marker == 0xffd8:\n                data = data[2:]\n            elif marker == 0xffd9:\n                return\n            elif marker == 0xffda:\n                data = data[-2:]\n            else:\n                lenchunk, = unpack(\">H\", data[2:4])\n                data = data[2+lenchunk:]            \n            if len(data)==0:\n                break   \n# JPEG binary marker mappings (verify integrity of jpeg file)\nmarker_mapping = {\n    0xffd8: \"Start of Image\",\n    0xffe0: \"Application Default Header\",\n    0xffdb: \"Quantization Table\",\n    0xffc0: \"Start of Frame\",\n    0xffc4: \"Define Huffman Table\",\n    0xffda: \"Start of Scan\",\n    0xffd9: \"End of Image\"\n}\n\n\nassert os.path.exists(BASE_DS_PATH), 'Given path does not exist. Provide a valid one'\ncorrupted = []\n\nfor root, subdirectories, files in tqdm(os.walk(BASE_DS_PATH)):\n    for im_file in files:\n        file_path = os.path.join(root, im_file)\n        try:\n            image = JPEG(file_path) \n            image.decode()   \n        except:\n          # Image is corrupted -> removing\n            if os.path.exists(file_path):\n            os.remove(file_path)\n            print(f\"Removed {file_path} - Corrupted image\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not DS_FROM_DIR:\n    # Setup data-generators\n    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rescale = 1./255., \n        rotation_range = 40, \n        zoom_range = 0.2, \n        horizontal_flip = True\n    )\n\n    valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n       rescale = 1./255.\n    )\n    \n\n    # Flow from directory data loaders\n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_SPLIT_PATH,\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        target_size=(IMAGE_SIZE, IMAGE_SIZE))\n    valid_generator = valid_datagen.flow_from_directory(\n        EVAL_SPLIT_PATH,\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE))\n\nelse:\n    # Setup data-loaders using DS_FROM_DIR\n    train_generator = tf.keras.preprocessing.image_dataset_from_directory(\n        TRAIN_SPLIT_PATH,\n        validation_split=0.1,\n        subset=\"training\",\n        seed=DATASET_SEED,\n        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n    )\n\n    valid_generator = tf.keras.preprocessing.image_dataset_from_directory(\n        EVAL_SPLIT_PATH,\n        validation_split=0.2,\n        subset=\"validation\",\n        seed=DATASET_SEED,\n        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n    )\n\nif DEBUG:\n  # Visualize a few imagess\n  plt.figure(figsize=(10, 10))\n    for images, labels in train_ds.take(1):\n        for i in range(100):\n            ax = plt.subplot(3, 3, i + 1)\n            plt.imshow(images[i].numpy().astype(\"uint8\"))\n            plt.title(int(labels[i]))\n            plt.axis(\"off\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select way to construct dataset, using flow-from-dir or image-dataset\nif DS_FROM_DIR:\n    # Apply augmentations over data loader\n    train_dataset = train_dataset.map(\n      lambda x, y: (augmentation_pipeline(x, training=True), y))\n    \n        # Convert generators to dataset loaders\n    train_dataset = tf.data.Dataset.from_generator(train_generator)\n    valid_dataset = tf.data.Dataset.from_generator(valid_generator)\n\n    # Prefetch data\n    train_dataset = train_dataset.prefetch(buffer_size=BATCH_SIZE)\n    valid_dataset = valid_dataset.prefetch(buffer_size=BATCH_SIZE)\n\n\n# Compile model with sparse-categorical (multi-class)\nmodel.compile(\n    optimizer='adam',\n    loss= 'categorical_crossentropy',\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(\n  train_generator,\n  validation_data=valid_generator,\n  epochs=EPOCHS,\n  callbacks=[chkpt_callback]\n)\n\nmodel.save(\"saved_model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See training stats\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}